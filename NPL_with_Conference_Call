---
title: "R Notebook"
output: html_notebook
---

##Exercicios do curso de "Applying Data Analytics in Accounting por Universidade de Illinois em Urbana-ChampaignUniversidade de Illinois em Urbana-Champaign
#Feito em RStudio
#NPL with Conference Call

# Install packages
```{r}
library(tidyverse)
library(tidytext)
library(SnowballC)
library(wordcloud)
library(Rcpp)
```

# Load one conference call as a text file
```{r}
save <- read_file(r"(C:\Users\Ricardoxx\Desktop\Erika\Auditoria\SAVEQ22020.txt)")
```

# Look at text
```{r}
# explore

summary(save)

# view
save %>% stringr::str_sub(1,500)

# count characters
save %>% stringr::str_count()

# detect keywords
save %>% stringr::str_detect(c('pandemic', 'Pandemic', 'covid', 'Covid', 'covid-19', 'Covid-19', 'covid 19', 'COVID'))

# count the number of matches of a substring
save %>% stringr::str_count('COVID')

# where is this keyword mentioned?
save %>% stringr::str_locate_all('COVID')

# view surrounding text (requires regex=regular expression {}(){} period # surrounding)
save %>% stringr::str_extract_all(".{50}(COVID).{50}")
```

# Tokenize the text
```{r}
# change to a tibble (tidy dataframe)
tokens <- tibble(save)

# Tokenize
tokens <- tokens %>% tidytext::unnest_tokens(output = word, input=save, token='words', to_lower=TRUE)

# add order of the words
tokens <- tokens %>% mutate(order = row_number())

# count tokens
tokens %>% nrow()

# first few words
tokens[1:15, ]

# count the number of matches of a substring

tokens %>% dplyr::filter(word == str_sub('covid')) %>% count()

# where is this keyword mentioned?
tokens %>% dplyr::filter(word == str_sub('covid'))

```
# Remove stop words
```{r}
# look at the most important frequent words

tokens %>%
  group_by(word) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>%
  filter(count > 50) %>%
  mutate(token = reorder(word, count)) %>%
  ggplot(aes(x=count, y=token)) + geom_col()

# Load custom stopwords
custom_stop_words <- read_csv(r"(C:\Users\Ricardoxx\Desktop\Erika\Auditoria\stop_words_list.csv)", col_names = FALSE)

# remove stop words
tokens <- tokens %>%
  anti_join(custom_stop_words, by = c('word'='X1'))

tokens %>% nrow()

tokens %>%
  group_by()

tokens %>%
  group_by(word) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>%
  filter(count > 20) %>%
  mutate(token = reorder(word, count)) %>%
  ggplot(aes(x=count, y=token)) + geom_col()


```
#Stemming and Lemmatizing
```{r}
# look at similar words
arrange(tokens, word)[316:325, ]

# install package ('snowballc')
stemmed <- tokens %>% mutate(stem = SnowballC::wordStem(word))

# look at similar words now
arrange(stemmed, word) [316:325, ]

stemmed %>%
  group_by(stem) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>%
  filter(count > 20) %>%
  mutate(token = reorder(stem, count)) %>%
  ggplot(aes(x=count, y=token)) + geom_col()

```
# Key words
```{r}
set.seed(77)

stemmed %>%
  group_by(word) %>%
  summarize(count = n()) %>%
  with(wordcloud(words = word, freq = count, min.freq = 1, max.words = 100, random.order = F, rot.per = 0.30, colors = brewer.pal(8, "Dark2")))
```

# Sentiment Total
```{r}
# load finance sentiment list and explore it
lm_dict <- tidytext::get_sentiments('loughran')

# view dictionary
lm_dict %>% group_by(sentiment) %>% summarize(count = n())

# Add sentiment
sentimented <- stemmed %>%
  inner_join(lm_dict, by = 'word')

# Explore totals
sentimented %>%
  group_by(sentiment) %>%
  summarize(count = n(), percent = count/nrow(sentimented))

sentimented %>%
  group_by(sentiment) %>%
  summarize(count = n(), percent = count/nrow(sentimented)) %>%
  ggplot(aes(x='', y=percent, fill=sentiment)) +
  geom_bar(width = 1, stat = 'identity')

```
